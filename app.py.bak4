import streamlit as st
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend
import seaborn as sns
from scipy import stats
from modules.data_processing import DataProcessor
from modules.advanced_data_processing import AdvancedDataProcessor
from modules.prediction import Predictor
from modules.risk_assessment import RiskAssessor
from utils.data_handler import DataHandler
from utils.visualization import Visualizer
from utils.database import DatabaseHandler

# Set page configuration
st.set_page_config(
    page_title="Data Analysis Platform",
    page_icon="üìä",
    layout="wide"
)

# Initialize session state variables if they don't exist
if 'original_data' not in st.session_state:
    st.session_state.original_data = None
if 'interpolated_data' not in st.session_state:
    st.session_state.interpolated_data = None
if 'data' not in st.session_state:  # Active data for processing
    st.session_state.data = None
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = None
if 'predictions' not in st.session_state:
    st.session_state.predictions = None
if 'risk_assessment' not in st.session_state:
    st.session_state.risk_assessment = None
if 'selected_features' not in st.session_state:
    st.session_state.selected_features = []
if 'target_column' not in st.session_state:
    st.session_state.target_column = None
if 'visualization_type' not in st.session_state:
    st.session_state.visualization_type = None
if 'active_dataset' not in st.session_state:
    st.session_state.active_dataset = "None"
    
# Advanced data processing state variables
if 'interpolated_result' not in st.session_state:
    st.session_state.interpolated_result = None
if 'cgan_results' not in st.session_state:
    st.session_state.cgan_results = None
if 'distribution_test_results' not in st.session_state:
    st.session_state.distribution_test_results = None
if 'outlier_results' not in st.session_state:
    st.session_state.outlier_results = None
if 'convergence_datasets' not in st.session_state:
    st.session_state.convergence_datasets = []
if 'convergence_iterations' not in st.session_state:
    st.session_state.convergence_iterations = 0

# Initialize modules
data_handler = DataHandler()
data_processor = DataProcessor()
advanced_processor = AdvancedDataProcessor()
predictor = Predictor()
risk_assessor = RiskAssessor()
visualizer = Visualizer()
db_handler = DatabaseHandler()

# Application title and description
st.title("Integrated Data Analysis Platform")
st.markdown("""
    This platform provides an integrated workflow for data analysis, from data import to visualization.
    All steps are available in one interface for easier access and navigation.
""")

# Main container
main_container = st.container()

with main_container:
    # Create tabs for each section of the workflow
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "1Ô∏è‚É£ Data Import", 
        "2Ô∏è‚É£ Data Processing", 
        "3Ô∏è‚É£ Prediction", 
        "4Ô∏è‚É£ Risk Assessment", 
        "5Ô∏è‚É£ Visualization",
        "6Ô∏è‚É£ Database"
    ])
    
    # 1. DATA IMPORT TAB
    with tab1:
        st.header("Data Import")
        
        # Data import section with two columns for original and interpolated data
        st.subheader("Import Multiple Datasets")
        
        # Import methods tabs
        import_tabs = st.tabs(["Upload Files", "Load from Database"])
        
        # TAB: UPLOAD FILES
        with import_tabs[0]:
            st.markdown("""
            Import both your original data and data to be interpolated to compare distributions and verify interpolation accuracy.
            """)
            
            col1, col2 = st.columns(2)
            
            # ORIGINAL DATA IMPORT
            with col1:
                st.subheader("Original Data")
                original_file = st.file_uploader("Upload Original Data (CSV/Excel)", type=["csv", "xlsx", "xls"], key="original_data_uploader")
                
                if original_file is not None:
                    try:
                        st.session_state.original_data = data_handler.import_data(original_file)
                        st.success(f"Original data imported: {st.session_state.original_data.shape[0]} rows, {st.session_state.original_data.shape[1]} columns")
                        
                        # Show data preview
                        st.write("Preview:")
                        st.dataframe(st.session_state.original_data.head())
                        
                    except Exception as e:
                        st.error(f"Error importing original data: {e}")
            
            # INTERPOLATED DATA IMPORT
            with col2:
                st.subheader("Data to Interpolate")
                interpolated_file = st.file_uploader("Upload Data for Interpolation (CSV/Excel)", type=["csv", "xlsx", "xls"], key="interpolated_data_uploader")
                
                if interpolated_file is not None:
                    try:
                        st.session_state.interpolated_data = data_handler.import_data(interpolated_file)
                        st.success(f"Interpolation data imported: {st.session_state.interpolated_data.shape[0]} rows, {st.session_state.interpolated_data.shape[1]} columns")
                        
                        # Show data preview
                        st.write("Preview:")
                        st.dataframe(st.session_state.interpolated_data.head())
                        
                    except Exception as e:
                        st.error(f"Error importing interpolation data: {e}")
        
        # TAB: LOAD FROM DATABASE
        with import_tabs[1]:
            st.markdown("""
            Load saved datasets from the database to continue your analysis.
            """)
            
            # Check if database is available
            if not hasattr(db_handler, 'db_available') or not db_handler.db_available:
                st.error("‚ö†Ô∏è Database connection is not available. Cannot load data from database.")
                st.info("Please check your database connection settings or continue using file upload.")
            else:
                # First add a "Load Both Datasets" section at the top
                st.subheader("Load Both Datasets with One Click")
                
                try:
                    # Get list of datasets from database
                    all_datasets = db_handler.list_datasets()
                    
                    if not all_datasets:
                        st.info("No datasets found in the database. Please save some datasets first.")
                    else:
                        # Create a formatted selectbox for datasets
                        dataset_options = [(ds['id'], f"{ds['name']} ({ds['data_type']}, {ds['row_count']}x{ds['column_count']})") 
                                          for ds in all_datasets]
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            original_dataset = st.selectbox(
                                "Select dataset for Original Data:",
                                options=dataset_options,
                                format_func=lambda x: x[1],
                                key="original_combined_select"
                            )
                        
                        with col2:
                            interpolated_dataset = st.selectbox(
                                "Select dataset for Data to Interpolate:",
                                options=dataset_options,
                                format_func=lambda x: x[1],
                                key="interpolated_combined_select"
                            )
                        
                        if st.button("Load Both Datasets", key="load_both_btn"):
                            try:
                                # Load the original dataset
                                original_df = db_handler.load_dataset(dataset_id=original_dataset[0])
                                st.session_state.original_data = original_df
                                
                                # Load the interpolated dataset
                                interpolated_df = db_handler.load_dataset(dataset_id=interpolated_dataset[0])
                                st.session_state.interpolated_data = interpolated_df
                                
                                # Success message
                                st.success(f"Both datasets loaded successfully!")
                                
                                # Show data previews
                                st.write("Original Data Preview:")
                                st.dataframe(original_df.head())
                                
                                st.write("Data to Interpolate Preview:")
                                st.dataframe(interpolated_df.head())
                                
                                # Set active dataset to the original data
                                st.session_state.data = original_df
                                st.session_state.active_dataset = "Original Data"
                                st.info("Original data set as active dataset for analysis.")
                                
                            except Exception as e:
                                st.error(f"Error loading datasets: {e}")
                
                except Exception as e:
                    st.error(f"Error accessing database: {e}")
                
                # Add separator
                st.markdown("---")
                st.markdown("### Load Individual Datasets")
                
                col1, col2 = st.columns(2)
                
                # LOAD AS ORIGINAL DATA
                with col1:
                    st.subheader("Load as Original Data")
                    
                    try:
                        # Get list of datasets from database
                        all_datasets = db_handler.list_datasets()
                        
                        if not all_datasets:
                            st.info("No datasets found in the database. Please save some datasets first.")
                        else:
                            # Create a formatted selectbox for datasets
                            dataset_options = [(ds['id'], f"{ds['name']} ({ds['data_type']}, {ds['row_count']}x{ds['column_count']})") 
                                              for ds in all_datasets]
                            
                            selected_dataset = st.selectbox(
                                "Select dataset to load as Original Data:",
                                options=dataset_options,
                                format_func=lambda x: x[1],
                                key="original_data_db_select"
                            )
                            
                            if st.button("Load as Original Data", key="load_original_btn"):
                                try:
                                    # Load the selected dataset
                                    loaded_df = db_handler.load_dataset(dataset_id=selected_dataset[0])
                                    st.session_state.original_data = loaded_df
                                    
                                    st.success(f"Dataset loaded as Original Data: {loaded_df.shape[0]} rows, {loaded_df.shape[1]} columns")
                                    
                                    # Show data preview
                                    st.write("Preview:")
                                    st.dataframe(loaded_df.head())
                                    
                                except Exception as e:
                                    st.error(f"Error loading dataset: {e}")
                    
                    except Exception as e:
                        st.error(f"Error accessing database: {e}")
                
                # LOAD AS INTERPOLATED DATA
                with col2:
                    st.subheader("Load as Data to Interpolate")
                    
                    try:
                        # Get list of datasets from database
                        all_datasets = db_handler.list_datasets()
                        
                        if not all_datasets:
                            st.info("No datasets found in the database. Please save some datasets first.")
                        else:
                            # Create a formatted selectbox for datasets
                            dataset_options = [(ds['id'], f"{ds['name']} ({ds['data_type']}, {ds['row_count']}x{ds['column_count']})") 
                                              for ds in all_datasets]
                            
                            selected_dataset = st.selectbox(
                                "Select dataset to load as Data to Interpolate:",
                                options=dataset_options,
                                format_func=lambda x: x[1],
                                key="interpolated_data_db_select"
                            )
                            
                            if st.button("Load as Data to Interpolate", key="load_interpolated_btn"):
                                try:
                                    # Load the selected dataset
                                    loaded_df = db_handler.load_dataset(dataset_id=selected_dataset[0])
                                    st.session_state.interpolated_data = loaded_df
                                    
                                    st.success(f"Dataset loaded as Data to Interpolate: {loaded_df.shape[0]} rows, {loaded_df.shape[1]} columns")
                                    
                                    # Show data preview
                                    st.write("Preview:")
                                    st.dataframe(loaded_df.head())
                                    
                                except Exception as e:
                                    st.error(f"Error loading dataset: {e}")
                    
                    except Exception as e:
                        st.error(f"Error accessing database: {e}")
        
        # Select active dataset for analysis
        st.subheader("Select Active Dataset")
        
        dataset_options = ["None"]
        if st.session_state.original_data is not None:
            dataset_options.append("Original Data")
        if st.session_state.interpolated_data is not None:
            dataset_options.append("Interpolated Data")
            
        st.session_state.active_dataset = st.radio(
            "Select which dataset to use for analysis:",
            dataset_options
        )
        
        # Set the active dataset
        if st.session_state.active_dataset == "Original Data":
            st.session_state.data = st.session_state.original_data
            st.success("Original data set as active dataset for analysis.")
        elif st.session_state.active_dataset == "Interpolated Data":
            st.session_state.data = st.session_state.interpolated_data
            st.success("Interpolated data set as active dataset for analysis.")
        else:
            st.session_state.data = None
            
        # Data Comparison (if both datasets are available)
        if st.session_state.original_data is not None and st.session_state.interpolated_data is not None:
            st.subheader("Dataset Comparison")
            
            with st.expander("Compare Dataset Statistics"):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("Original Data Statistics")
                    st.write(st.session_state.original_data.describe())
                    
                with col2:
                    st.write("Interpolated Data Statistics")
                    st.write(st.session_state.interpolated_data.describe())
                    
            # Basic comparison metrics can be added here
            st.write("Shape Comparison:")
            st.write(f"Original: {st.session_state.original_data.shape} | Interpolated: {st.session_state.interpolated_data.shape}")
                
            # If columns match, show correlation
            if set(st.session_state.original_data.columns) == set(st.session_state.interpolated_data.columns):
                common_cols = list(set(st.session_state.original_data.select_dtypes(include=np.number).columns) & 
                                set(st.session_state.interpolated_data.select_dtypes(include=np.number).columns))
                
                if common_cols:
                    st.write("Compare Distribution of a Column:")
                    selected_col = st.selectbox("Select column to compare:", common_cols, key="compare_column_select")
                    
                    if selected_col:
                        # Create a comparison histogram
                        fig = plt.figure(figsize=(10, 6))
                        plt.hist(st.session_state.original_data[selected_col], alpha=0.5, label='Original')
                        plt.hist(st.session_state.interpolated_data[selected_col], alpha=0.5, label='Interpolated')
                        plt.legend()
                        plt.title(f'Distribution Comparison: {selected_col}')
                        plt.xlabel(selected_col)
                        plt.ylabel('Frequency')
                        st.pyplot(fig)
    
    # 2. DATA PROCESSING TAB
    with tab2:
        st.header("Data Processing")
        
        if (st.session_state.original_data is None and st.session_state.interpolated_data is None):
            st.warning("No data available. Please import data in the Data Import tab.")
        else:
            # Create processing tabs for Basic and Advanced processing
            processing_tabs = st.tabs(["Basic Processing", "Advanced Processing"])
            
            # BASIC PROCESSING TAB
            with processing_tabs[0]:
                st.subheader("Basic Data Processing")
                
                if st.session_state.data is None:
                    st.warning("No active dataset selected. Please select a dataset in the Data Import tab.")
                else:
                    # Initialize the processed data output if needed
                    if 'basic_processed_outputs' not in st.session_state:
                        st.session_state.basic_processed_outputs = {}
                
                    # Data preview
                    st.write("Active Dataset Preview:")
                    st.dataframe(st.session_state.data.head())
                    
                    # Create tabs for different processing categories
                    basic_tabs = st.tabs(["Feature Selection", "Data Cleaning", "Transformation", "Interpolation", "Results"])
                    
                    # 1. FEATURE SELECTION TAB
                    with basic_tabs[0]:
                        st.write("### Feature Selection")
                        st.write("Select features and target variable for processing:")
                        
                        # Get all columns
                        all_columns = st.session_state.data.columns.tolist()
                        
                        # Feature selection
                        st.session_state.selected_features = st.multiselect(
                            "Features to include in processing",
                            all_columns,
                            default=st.session_state.selected_features if st.session_state.selected_features else all_columns
                        )
                        
                        # Target variable selection
                        st.write("Select target variable (for prediction):")
                        
                        # Handle target column selection
                        current_index = 0  # Default to 'None'
                        if st.session_state.target_column is not None and st.session_state.target_column != "None":
                            try:
                                current_index = all_columns.index(st.session_state.target_column) + 1
                            except ValueError:
                                # Target column not in the list, reset to None
                                st.session_state.target_column = None
                                
                        st.session_state.target_column = st.selectbox(
                            "Target Variable",
                            ["None"] + all_columns,
                            index=current_index,
                            key="target_column_select"
                        )
                        
                        # Apply feature selection
                        if st.button("Apply Feature Selection", key="apply_feature_btn"):
                            # Get a subset of data with selected features
                            if st.session_state.selected_features:
                                subset_data = st.session_state.data[st.session_state.selected_features].copy()
                                
                                # Store the subset data
                                st.session_state.basic_processed_outputs['selected_features'] = subset_data
                                
                                st.success(f"Selected {len(st.session_state.selected_features)} features")
                                st.write("Preview of selected features:")
                                st.dataframe(subset_data.head())
                            else:
                                st.warning("No features selected.")
                    
                    # 2. DATA CLEANING TAB
                    with basic_tabs[1]:
                        st.write("### Data Cleaning")
                        st.write("Options for cleaning and preparing the data:")
                        
                        # Choose data source
                        data_source_options = ["Original Data"]
                        if st.session_state.basic_processed_outputs:
                            data_source_options.extend(list(st.session_state.basic_processed_outputs.keys()))
                        
                        data_source = st.selectbox(
                            "Select data source for cleaning:",
                            data_source_options,
                            key="cleaning_data_source"
                        )
                        
                        # Get the selected data
                        if data_source == "Original Data":
                            cleaning_data = st.session_state.data
                        else:
                            cleaning_data = st.session_state.basic_processed_outputs[data_source]
                        
                        # Missing value handling
                        st.write("#### Missing Value Handling")
                        handle_missing = st.checkbox("Handle missing values", key="handle_missing")
                        
                        if handle_missing:
                            missing_method = st.radio(
                                "Method for handling missing values:",
                                ["Remove rows", "Remove columns", "Mean imputation", "Median imputation", "Mode imputation"],
                                key="missing_method"
                            )
                            
                            # Show missing value statistics
                            missing_df = pd.DataFrame({
                                'Column': cleaning_data.columns,
                                'Missing Values': cleaning_data.isna().sum().values,
                                'Percentage': (cleaning_data.isna().sum().values / len(cleaning_data) * 100).round(2)
                            }).sort_values(by='Missing Values', ascending=False)
                            
                            st.write("#### Missing Value Statistics")
                            st.dataframe(missing_df)
                        
                        # Duplicate handling
                        st.write("#### Duplicate Handling")
                        remove_duplicates = st.checkbox("Remove duplicate rows", key="remove_duplicates")
                        
                        if remove_duplicates:
                            # Show duplicate information
                            duplicate_count = cleaning_data.duplicated().sum()
                            if duplicate_count > 0:
                                st.write(f"Found {duplicate_count} duplicate rows ({duplicate_count/len(cleaning_data)*100:.2f}% of data)")
                            else:
                                st.write("No duplicate rows found in the data.")
                        
                        # Apply cleaning button
                        if st.button("Apply Data Cleaning", key="apply_cleaning_btn"):
                            # Create a copy of the data
                            cleaned_data = cleaning_data.copy()
                            cleaning_steps = []
                            
                            try:
                                # Handle missing values
                                if handle_missing:
                                    if missing_method == "Remove rows":
                                        rows_before = len(cleaned_data)
                                        cleaned_data = cleaned_data.dropna()
                                        rows_removed = rows_before - len(cleaned_data)
                                        cleaning_steps.append(f"Removed {rows_removed} rows with missing values")
                                    
                                    elif missing_method == "Remove columns":
                                        cols_before = len(cleaned_data.columns)
                                        # Remove columns with more than 50% missing values
                                        threshold = 0.5
                                        cleaned_data = cleaned_data.loc[:, cleaned_data.isna().mean() < threshold]
                                        cols_removed = cols_before - len(cleaned_data.columns)
                                        cleaning_steps.append(f"Removed {cols_removed} columns with >50% missing values")
                                    
                                    elif missing_method == "Mean imputation":
                                        # Only apply to numeric columns
                                        numeric_cols = cleaned_data.select_dtypes(include=['number']).columns
                                        for col in numeric_cols:
                                            cleaned_data[col] = cleaned_data[col].fillna(cleaned_data[col].mean())
                                        cleaning_steps.append(f"Filled missing values in numeric columns with mean")
                                    
                                    elif missing_method == "Median imputation":
                                        # Only apply to numeric columns
                                        numeric_cols = cleaned_data.select_dtypes(include=['number']).columns
                                        for col in numeric_cols:
                                            cleaned_data[col] = cleaned_data[col].fillna(cleaned_data[col].median())
                                        cleaning_steps.append(f"Filled missing values in numeric columns with median")
                                    
                                    elif missing_method == "Mode imputation":
                                        # Apply to all columns
                                        for col in cleaned_data.columns:
                                            # Check if mode exists
                                            mode_values = cleaned_data[col].mode()
                                            if not mode_values.empty:
                                                cleaned_data[col] = cleaned_data[col].fillna(mode_values[0])
                                        cleaning_steps.append(f"Filled missing values in all columns with mode")
                                
                                # Handle duplicates
                                if remove_duplicates:
                                    rows_before = len(cleaned_data)
                                    cleaned_data = cleaned_data.drop_duplicates()
                                    rows_removed = rows_before - len(cleaned_data)
                                    cleaning_steps.append(f"Removed {rows_removed} duplicate rows")
                                
                                # Store the cleaned data
                                st.session_state.basic_processed_outputs['cleaned_data'] = cleaned_data
                                
                                # Display summary
                                st.success("Data cleaning applied successfully!")
                                st.write("#### Cleaning Summary")
                                for step in cleaning_steps:
                                    st.write(f"- {step}")
                                
                                # Display shape information
                                st.write(f"Original shape: {cleaning_data.shape[0]} rows, {cleaning_data.shape[1]} columns")
                                st.write(f"Cleaned shape: {cleaned_data.shape[0]} rows, {cleaned_data.shape[1]} columns")
                                
                                # Display cleaned data
                                st.write("#### Cleaned Data Preview")
                                st.dataframe(cleaned_data.head())
                                
                            except Exception as e:
                                st.error(f"Error during data cleaning: {e}")
                    
                    # 3. TRANSFORMATION TAB
                    with basic_tabs[2]:
                        st.write("### Data Transformation")
                        st.write("Apply transformations to prepare data for modeling:")
                        
                        # Choose data source
                        data_source_options = ["Original Data"]
                        if st.session_state.basic_processed_outputs:
                            data_source_options.extend(list(st.session_state.basic_processed_outputs.keys()))
                        
                        data_source = st.selectbox(
                            "Select data source for transformation:",
                            data_source_options,
                            key="transform_data_source"
                        )
                        
                        # Get the selected data
                        if data_source == "Original Data":
                            transform_data = st.session_state.data
                        else:
                            transform_data = st.session_state.basic_processed_outputs[data_source]
                        
                        # Normalization/Scaling
                        st.write("#### Normalization & Scaling")
                        normalize_data = st.checkbox("Normalize/Scale numerical features", key="normalize_data")
                        
                        if normalize_data:
                            norm_method = st.radio(
                                "Normalization method:",
                                ["Min-Max Scaling", "Standard Scaling", "Robust Scaling"],
                                key="norm_method"
                            )
                        
                        # Apply transformations button
                        if st.button("Apply Transformations", key="apply_transform_btn"):
                            # Create a copy of the data
                            transformed_data = transform_data.copy()
                            transform_steps = []
                            
                            try:
                                # Apply normalizations
                                if normalize_data:
                                    # Get numeric columns
                                    numeric_cols = transformed_data.select_dtypes(include=['number']).columns.tolist()
                                    
                                    if numeric_cols:
                                        # Process data using existing data_processor
                                        transformed_data = data_processor.process_data(
                                            transformed_data,
                                            target_column=st.session_state.target_column if st.session_state.target_column != "None" else None,
                                            handle_missing=False,
                                            remove_duplicates=False,
                                            normalize_data=True,
                                            norm_method=norm_method
                                        )
                                        
                                        transform_steps.append(f"Applied {norm_method} to {len(numeric_cols)} numeric columns")
                                    else:
                                        st.warning("No numeric columns found for normalization.")
                                
                                # Store the transformed data
                                st.session_state.basic_processed_outputs['transformed_data'] = transformed_data
                                
                                # Display summary
                                st.success("Data transformations applied successfully!")
                                st.write("#### Transformation Summary")
                                for step in transform_steps:
                                    st.write(f"- {step}")
                                
                                # Display shape information
                                st.write(f"Original shape: {transform_data.shape[0]} rows, {transform_data.shape[1]} columns")
                                st.write(f"Transformed shape: {transformed_data.shape[0]} rows, {transformed_data.shape[1]} columns")
                                
                                # Display the transformed data
                                st.write("#### Transformed Data Preview")
                                st.dataframe(transformed_data.head())
                                
                            except Exception as e:
                                st.error(f"Error during data transformation: {e}")
                    
                    # 4. INTERPOLATION TAB
                    with basic_tabs[3]:
                        st.write("### Data Interpolation")
                        st.write("Fill missing values using various interpolation methods:")
                        
                        # Choose data source
                        data_source_options = ["Original Data"]
                        if st.session_state.basic_processed_outputs:
                            data_source_options.extend(list(st.session_state.basic_processed_outputs.keys()))
                        
                        data_source = st.selectbox(
                            "Select data source for interpolation:",
                            data_source_options,
                            key="interp_data_source"
                        )
                        
                        # Get the selected data
                        if data_source == "Original Data":
                            interp_data = st.session_state.data
                        else:
                            interp_data = st.session_state.basic_processed_outputs[data_source]
                        
                        # Check if there are any missing values
                        missing_counts = interp_data.isna().sum()
                        total_missing = missing_counts.sum()
                        
                        if total_missing == 0:
                            st.info("The selected data does not contain any missing values. Consider creating artificial missing values for testing interpolation.")
                            
                            # Option to create artificial missing values
                            create_missing = st.checkbox("Create artificial missing values", key="create_artificial_missing")
                            
                            if create_missing:
                                # Choose percentage of missing values
                                missing_pct = st.slider(
                                    "Percentage of values to make missing:",
                                    min_value=5,
                                    max_value=50,
                                    value=20,
                                    key="missing_pct"
                                )
                                
                                # Select columns where missing values will be created
                                missing_cols = st.multiselect(
                                    "Select columns where missing values will be created:",
                                    interp_data.columns.tolist(),
                                    key="missing_cols"
                                )
                                
                                # Create missing values
                                if st.button("Create Missing Values", key="create_missing_btn"):
                                    try:
                                        import numpy as np
                                        
                                        # Create a copy of the data
                                        data_with_missing = interp_data.copy()
                                        
                                        # Create missing values only in selected columns
                                        for col in missing_cols:
                                            # Calculate how many values to make missing
                                            n_missing = int(len(data_with_missing) * missing_pct / 100)
                                            
                                            # Create random indices for missing values
                                            missing_indices = np.random.choice(
                                                data_with_missing.index, 
                                                size=n_missing, 
                                                replace=False
                                            )
                                            
                                            # Set values as missing
                                            data_with_missing.loc[missing_indices, col] = np.nan
                                        
                                        # Store the data with artificial missing values
                                        st.session_state.basic_processed_outputs['data_with_missing'] = data_with_missing
                                        
                                        # Display the result
                                        st.success(f"Created artificial missing values in {len(missing_cols)} columns")
                                        st.write("#### Missing Value Counts After")
                                        missing_df = pd.DataFrame({
                                            'Column': data_with_missing.columns,
                                            'Missing Values': data_with_missing.isna().sum().values,
                                            'Percentage': (data_with_missing.isna().sum().values / len(data_with_missing) * 100).round(2)
                                        }).sort_values(by='Missing Values', ascending=False)
                                        
                                        st.dataframe(missing_df)
                                        
                                        # Update the interpolation data to use the one with missing values
                                        interp_data = data_with_missing
                                        
                                    except Exception as e:
                                        st.error(f"Error creating artificial missing values: {e}")
                        else:
                            # Display missing value statistics
                            st.write("#### Missing Value Statistics")
                            missing_df = pd.DataFrame({
                                'Column': interp_data.columns,
                                'Missing Values': interp_data.isna().sum().values,
                                'Percentage': (interp_data.isna().sum().values / len(interp_data) * 100).round(2)
                            }).sort_values(by='Missing Values', ascending=False)
                            
                            st.dataframe(missing_df)
                        
                        # Interpolation options
                        st.write("#### Interpolation Method")
                        interp_method = st.radio(
                            "Select interpolation method:",
                            ["Linear Interpolation", "Polynomial Interpolation", "Spline Interpolation", "Nearest Neighbor", "MCMC (Monte Carlo)"],
                            key="interp_method"
                        )
                        
                        # Method-specific options
                        if interp_method == "Polynomial Interpolation":
                            poly_order = st.slider(
                                "Polynomial order:",
                                min_value=1,
                                max_value=5,
                                value=2,
                                key="poly_order"
                            )
                        
                        elif interp_method == "Spline Interpolation":
                            spline_order = st.slider(
                                "Spline order:",
                                min_value=1,
                                max_value=5,
                                value=3,
                                key="spline_order"
                            )
                        
                        elif interp_method == "MCMC (Monte Carlo)":
                            st.info("This will use the Advanced Processing MCMC method.")
                            
                            # MCMC specific parameters
                            num_samples = st.slider(
                                "Number of MCMC samples", 
                                min_value=100, 
                                max_value=1000, 
                                value=500, 
                                step=100
                            )
                            
                            chains = st.slider(
                                "Number of MCMC chains", 
                                min_value=1, 
                                max_value=4, 
                                value=2
                            )
                        
                        # Select columns for interpolation
                        interp_columns = st.multiselect(
                            "Select columns for interpolation (leave empty for all columns with missing values):",
                            interp_data.columns.tolist(),
                            key="interp_columns"
                        )
                        
                        # If no columns are selected, use all columns with missing values
                        if not interp_columns:
                            interp_columns = missing_counts[missing_counts > 0].index.tolist()
                        
                        # Apply interpolation button
                        if st.button("Apply Interpolation", key="apply_interp_btn"):
                            try:
                                # Create a copy of the data
                                interpolated_data = interp_data.copy()
                                
                                # Apply the selected interpolation method
                                if interp_method == "Linear Interpolation":
                                    # Apply linear interpolation to selected columns
                                    for col in interp_columns:
                                        interpolated_data[col] = interpolated_data[col].interpolate(method='linear')
                                    
                                    st.success(f"Applied linear interpolation to {len(interp_columns)} columns")
                                
                                elif interp_method == "Polynomial Interpolation":
                                    # Apply polynomial interpolation to selected columns
                                    for col in interp_columns:
                                        if pd.api.types.is_numeric_dtype(interpolated_data[col]):
                                            interpolated_data[col] = interpolated_data[col].interpolate(
                                                method='polynomial', order=poly_order
                                            )
                                    
                                    st.success(f"Applied polynomial interpolation (order {poly_order}) to {len(interp_columns)} columns")
                                
                                elif interp_method == "Spline Interpolation":
                                    # Apply spline interpolation to selected columns
                                    for col in interp_columns:
                                        if pd.api.types.is_numeric_dtype(interpolated_data[col]):
                                            interpolated_data[col] = interpolated_data[col].interpolate(
                                                method='spline', order=spline_order
                                            )
                                    
                                    st.success(f"Applied spline interpolation (order {spline_order}) to {len(interp_columns)} columns")
                                
                                elif interp_method == "Nearest Neighbor":
                                    # Apply nearest neighbor interpolation to selected columns
                                    for col in interp_columns:
                                        interpolated_data[col] = interpolated_data[col].interpolate(
                                            method='nearest'
                                        )
                                    
                                    st.success(f"Applied nearest neighbor interpolation to {len(interp_columns)} columns")
                                
                                elif interp_method == "MCMC (Monte Carlo)":
                                    # Use the advanced processor for MCMC interpolation
                                    with st.spinner("Running MCMC interpolation... (this may take a while)"):
                                        interpolated_data = advanced_processor.mcmc_interpolation(
                                            interpolated_data,
                                            num_samples=num_samples,
                                            chains=chains
                                        )
                                    
                                    st.success(f"Applied MCMC interpolation to {len(interp_columns)} columns")
                                
                                # Store the interpolated data
                                st.session_state.basic_processed_outputs['interpolated_data'] = interpolated_data
                                
                                # Store in the interpolated_data session state for compatibility with advanced processing
                                st.session_state.interpolated_data = interpolated_data
                                
                                # Display missing value statistics after interpolation
                                st.write("#### Missing Value Counts After Interpolation")
                                missing_after_df = pd.DataFrame({
                                    'Column': interpolated_data.columns,
                                    'Missing Values': interpolated_data.isna().sum().values,
                                    'Percentage': (interpolated_data.isna().sum().values / len(interpolated_data) * 100).round(2)
                                }).sort_values(by='Missing Values', ascending=False)
                                
                                st.dataframe(missing_after_df)
                                
                                # Display the interpolated data
                                st.write("#### Interpolated Data Preview")
                                st.dataframe(interpolated_data.head())
                                
                            except Exception as e:
                                st.error(f"Error during interpolation: {e}")
                    
                    # 5. RESULTS TAB
                    with basic_tabs[4]:
                        st.write("### Processing Results")
                        st.write("Review and set processed data for further analysis:")
                        
                        # Display available processed outputs
                        if st.session_state.basic_processed_outputs:
                            st.write("#### Available Processed Outputs")
                            
                            # Create a table to show available outputs
                            output_info = []
                            for output_name, output_data in st.session_state.basic_processed_outputs.items():
                                output_info.append({
                                    'Output Name': output_name,
                                    'Shape': f"{output_data.shape[0]} rows √ó {output_data.shape[1]} columns",
                                    'Missing Values': output_data.isna().sum().sum(),
                                    'Data Types': len(output_data.dtypes.unique())
                                })
                            
                            output_df = pd.DataFrame(output_info)
                            st.dataframe(output_df)
                            
                            # Select output to view
                            selected_output = st.selectbox(
                                "Select output to view:",
                                list(st.session_state.basic_processed_outputs.keys()),
                                key="view_output"
                            )
                            
                            # Display selected output
                            if selected_output:
                                output_data = st.session_state.basic_processed_outputs[selected_output]
                                
                                st.write(f"#### Preview of '{selected_output}'")
                                st.dataframe(output_data.head())
                                
                                # Options for setting data in session state
                                st.write("#### Set Data for Analysis")
                                
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    # Set as processed data (for prediction)
                                    if st.button(f"Use as Processed Data", key=f"use_processed_{selected_output}"):
                                        st.session_state.processed_data = output_data.copy()
                                        st.success(f"'{selected_output}' set as processed data for prediction!")
                                
                                with col2:
                                    # Set as interpolated data (for advanced processing)
                                    if st.button(f"Use for Advanced Processing", key=f"use_advanced_{selected_output}"):
                                        st.session_state.interpolated_data = output_data.copy()
                                        st.success(f"'{selected_output}' set as data for advanced processing!")
                                        
                            # Export options
                            st.write("#### Export Options")
                            
                            export_output = st.selectbox(
                                "Select output to export:",
                                list(st.session_state.basic_processed_outputs.keys()),
                                key="export_output"
                            )
                            
                            if export_output:
                                export_format = st.radio(
                                    "Export format:",
                                    ["CSV", "Excel"],
                                    key="export_format"
                                )
                                
                                output_data = st.session_state.basic_processed_outputs[export_output]
                                
                                try:
                                    if export_format == "CSV":
                                        data_bytes = data_handler.export_data(output_data, format='csv')
                                        st.download_button(
                                            label=f"Download {export_output} as CSV",
                                            data=data_bytes,
                                            file_name=f"{export_output}.csv",
                                            mime="text/csv"
                                        )
                                    else:  # Excel
                                        data_bytes = data_handler.export_data(output_data, format='excel')
                                        st.download_button(
                                            label=f"Download {export_output} as Excel",
                                            data=data_bytes,
                                            file_name=f"{export_output}.xlsx",
                                            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                                        )
                                except Exception as e:
                                    st.error(f"Error preparing download: {e}")
                            
                            # Save to database
                            st.write("#### Save to Database")
                            
                            db_output = st.selectbox(
                                "Select output to save to database:",
                                list(st.session_state.basic_processed_outputs.keys()),
                                key="db_output"
                            )
                            
                            if db_output:
                                save_name = st.text_input(
                                    "Dataset name:",
                                    value=f"{db_output}_{pd.Timestamp.now().strftime('%Y%m%d_%H%M')}",
                                    key="save_name"
                                )
                                
                                save_desc = st.text_area(
                                    "Description (optional):",
                                    key="save_desc"
                                )
                                
                                if st.button("Save to Database", key="save_to_db"):
                                    try:
                                        output_data = st.session_state.basic_processed_outputs[db_output]
                                        
                                        # Save to database
                                        result = db_handler.save_dataset(
                                            output_data,
                                            name=save_name,
                                            description=save_desc,
                                            data_type="processed"
                                        )
                                        
                                        if result:
                                            st.success(f"Successfully saved '{save_name}' to database with ID: {result}")
                                        else:
                                            st.error("Failed to save dataset to database")
                                    
                                    except Exception as e:
                                        st.error(f"Error saving to database: {e}")
                        else:
                            st.info("No processed outputs available. Please perform processing operations in the other tabs first.")
            
            # ADVANCED PROCESSING TAB
            with processing_tabs[1]:
                st.subheader("Advanced Data Processing")
                
                # Check if we have both original and interpolation data
                if st.session_state.original_data is None:
                    st.warning("No original data available. Please import original data in the Data Import tab.")
                elif st.session_state.interpolated_data is None and 'interpolated_result' not in st.session_state:
                    st.info("No data to interpolate detected. You can either:")
                    st.info("1. Import data to interpolate in the Data Import tab, or")
                    st.info("2. Use MCMC interpolation on your original data (with artificially created missing values)")
                    
                    # Option to create artificial missing values
                    create_missing = st.checkbox("Create artificial missing values in original data for interpolation demo", value=False)
                    
                    if create_missing:
                        missing_ratio = st.slider("Percentage of values to make missing", min_value=5, max_value=50, value=20) / 100
                        
                        if st.button("Create Missing Values Dataset"):
                            # Create a copy of original data with artificial missing values
                            import numpy as np
                            
                            data_with_missing = st.session_state.original_data.copy()
                            
                            # Only make numeric columns have missing values
                            numeric_cols = data_with_missing.select_dtypes(include=np.number).columns
                            
                            # Loop through numeric columns and set some values to NaN
                            for col in numeric_cols:
                                mask = np.random.random(len(data_with_missing)) < missing_ratio
                                data_with_missing.loc[mask, col] = np.nan
                            
                            # Store the result
                            st.session_state.interpolated_data = data_with_missing
                            
                            st.success("Created dataset with artificial missing values for interpolation!")
                            st.write("Preview of data with missing values:")
                            st.dataframe(data_with_missing.head())
                            
                            # Update active dataset if needed
                            if st.session_state.active_dataset == "None":
                                st.session_state.active_dataset = "Interpolated Data"
                                st.session_state.data = st.session_state.interpolated_data
                else:
                    # We have both datasets or previously interpolated result, show advanced processing options
                    
                    # Store both datasets for reference
                    original_data = st.session_state.original_data
                    
                    # Choose interpolated data source
                    interpolated_data = None
                    
                    # Get original data
                    if original_data is None:
                        st.warning("Original data is not available. Please import original data from the Data Import tab.")
                    
                    # Check if we have previously interpolated result
                    has_previous_result = 'interpolated_result' in st.session_state and st.session_state.interpolated_result is not None
                    has_interpolated_data = 'interpolated_data' in st.session_state and st.session_state.interpolated_data is not None
                    
                    if has_previous_result or has_interpolated_data:
                        # Choose data source options based on availability
                        source_options = []
                        if has_previous_result:
                            source_options.append("Use previously interpolated result")
                        if has_interpolated_data:
                            source_options.append("Use imported data for interpolation")
                        
                        if len(source_options) > 0:
                            interpolation_source = st.radio(
                                "Interpolated Data Source",
                                source_options,
                                index=0,
                                key="interpolation_source"
                            )
                            
                            if interpolation_source == "Use previously interpolated result" and has_previous_result:
                                interpolated_data = st.session_state.interpolated_result
                                st.success("Using previously interpolated result for analysis.")
                            elif has_interpolated_data:
                                interpolated_data = st.session_state.interpolated_data
                                st.success("Using imported data with missing values for interpolation.")
                    else:
                        st.info("No data available for interpolation. Please import data with missing values in the Data Import tab or create artificial missing values below.")
                    
                    # Create advanced processing options with tabs
                    advanced_options = st.tabs([
                        "Step 1: MCMC Interpolation", 
                        "Step 2: Modules Analysis",
                        "Step 3: CGAN Analysis", 
                        "Step 4: Distribution Testing", 
                        "Step 5: Outlier Detection"
                    ])
                    
                    # 1. MCMC INTERPOLATION TAB
                    with advanced_options[0]:
                        st.write("### MCMC-based Interpolation")
                        st.write("""
                        Markov Chain Monte Carlo (MCMC) interpolation uses Bayesian methods to fill missing values 
                        while accounting for uncertainty in the interpolated values.
                        """)
                        
                        # Interpolation parameters
                        with st.expander("Interpolation Parameters", expanded=True):
                            num_samples = st.slider("Number of MCMC samples", min_value=100, max_value=1000, value=500, step=100)
                            chains = st.slider("Number of MCMC chains", min_value=1, max_value=4, value=2)
                            
                            # Add control for the number of datasets to fill
                            st.write("#### Multiple Dataset Generation")
                            st.write("Generate multiple interpolated datasets with identical parameters for convergence testing:")
                            generate_multiple = st.checkbox("Generate multiple datasets", value=True)
                            
                            if generate_multiple:
                                num_datasets = st.slider("Number of datasets to generate", min_value=1, max_value=10, value=5, step=1)
                                dataset_prefix = st.text_input("Dataset name prefix", value="MCMC_Interpolation")
                        
                        # Run MCMC interpolation button
                        if st.button("Run MCMC Interpolation", key="mcmc_btn"):
                            try:
                                # First check if interpolated_data is None
                                if interpolated_data is None:
                                    st.error("No data available for interpolation. Please import or create data with missing values first.")
                                # Then check if we have missing values to interpolate
                                elif not interpolated_data.isna().any().any():
                                    st.warning("No missing values detected in the data. MCMC interpolation requires missing values.")
                                else:
                                    # Initialize datasets to store multiple results if needed
                                    generated_datasets = []
                                    
                                    # Determine how many datasets to generate
                                    iterations = num_datasets if generate_multiple else 1
                                    
                                    with st.spinner(f"Running MCMC interpolation for {iterations} dataset(s)... (this may take a while)"):
                                        # Generate the requested number of datasets
                                        for i in range(iterations):
                                            # Show progress for multiple datasets
                                            if generate_multiple:
                                                st.text(f"Generating dataset {i+1} of {iterations}...")
                                                
                                            # Run MCMC interpolation
                                            interpolated_result = advanced_processor.mcmc_interpolation(
                                                interpolated_data,
                                                num_samples=num_samples,
                                                chains=chains
                                            )
                                            
                                            # For the first dataset (or if only generating one), store as main result
                                            if i == 0:
                                                st.session_state.interpolated_result = interpolated_result
                                            
                                            # If generating multiple, add each to the results list with metadata
                                            if generate_multiple:
                                                dataset_info = {
                                                    'id': len(st.session_state.convergence_datasets) + i + 1,
                                                    'data': interpolated_result.copy(),
                                                    'convergence_scores': {},
                                                    'timestamp': pd.Timestamp.now(),
                                                    'name': f"{dataset_prefix}_{i+1}"
                                                }
                                                generated_datasets.append(dataset_info)
                                        
                                        # Show success message
                                        if generate_multiple:
                                            st.success(f"MCMC interpolation completed successfully for {iterations} datasets!")
                                        else:
                                            st.success("MCMC interpolation completed successfully!")
                                        
                                        # Add generated datasets to convergence analysis if requested
                                        if generate_multiple:
                                            if 'convergence_datasets' not in st.session_state:
                                                st.session_state.convergence_datasets = []
                                                st.session_state.convergence_iterations = 0
                                            
                                            # Add the datasets to the session state
                                            for dataset in generated_datasets:
                                                st.session_state.convergence_datasets.append(dataset)
                                                st.session_state.convergence_iterations += 1
                                            
                                            st.info(f"Added {iterations} datasets to the Modules Analysis. Please proceed to that tab for analysis.")
                                        
                                        # Display side-by-side comparison of before and after
                                        col1, col2 = st.columns(2)
                                        
                                        with col1:
                                            st.write("Before Interpolation:")
                                            st.dataframe(interpolated_data.head())
                                            
                                        with col2:
                                            st.write("After Interpolation:")
                                            st.dataframe(st.session_state.interpolated_result.head())
                                        
                                        # Show missing value counts before and after
                                        missing_before = interpolated_data.isna().sum().sum()
                                        missing_after = st.session_state.interpolated_result.isna().sum().sum()
                                        
                                        st.write(f"Missing values before: {missing_before}")
                                        st.write(f"Missing values after: {missing_after}")
                                        
                                        # Automatically set interpolated data as the active dataset
                                        st.session_state.data = st.session_state.interpolated_result
                                        st.session_state.active_dataset = "Interpolated Data"
                                        st.success("Interpolated result automatically set as active dataset for analysis.")
                                        
                                        # Add download button for interpolated data
                                        try:
                                            data_bytes = data_handler.export_data(st.session_state.interpolated_result, format='csv')
                                            st.download_button(
                                                label="Download Interpolated Data as CSV",
                                                data=data_bytes,
                                                file_name="mcmc_interpolated_data.csv",
                                                mime="text/csv"
                                            )
                                        except Exception as e:
                                            st.error(f"Error preparing download: {e}")
                            except Exception as e:
                                st.error(f"Error during MCMC interpolation: {e}")
                    
                    # 2. MODULES ANALYSIS TAB
                    with advanced_options[1]:
                        st.write("### Modules Analysis")
                        st.write("""
                        The Modules Analysis component provides a unified interface for analyzing datasets through multiple
                        analytical methods and evaluating them in parallel. Key benefits include:
                        1. Applying different analytical techniques to the same dataset
                        2. Comparing results across methods to ensure consistency
                        3. Evaluating convergence and reliability of interpolated data
                        """)
                        
                        # Check if we have MCMC interpolated result
                        if 'interpolated_result' not in st.session_state:
                            st.info("Please run MCMC interpolation first before performing modules analysis.")
                        else:
                            # Display core information about the imputation process
                            st.subheader("Imputation Statistics")
                            
                            # Get current imputed dataset if available
                            if 'interpolated_result' in st.session_state and st.session_state.interpolated_result is not None:
                                current_data = st.session_state.interpolated_result
                                
                                # Calculate imputation statistics
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    st.write("**Original Missing Data**")
                                    if 'original_missing_counts' in st.session_state:
                                        missing_counts = st.session_state.original_missing_counts
                                        total_missing = missing_counts.sum().sum()
                                        total_cells = missing_counts.size
                                        
                                        st.write(f"Total missing values: {total_missing}")
                                        st.write(f"Missing percentage: {total_missing/total_cells*100:.2f}%")
                                        
                                        # Show top columns with missing values
                                        missing_by_col = missing_counts.sum()
                                        if len(missing_by_col) > 0:
                                            top_missing = missing_by_col.sort_values(ascending=False).head(5)
                                            st.write("Columns with most missing values:")
                                            for col, count in top_missing.items():
                                                if count > 0:
                                                    st.write(f"- {col}: {count} values ({count/len(current_data)*100:.1f}%)")
                                    else:
                                        st.write("Original missing value information not available.")
                                
                                with col2:
                                    st.write("**Imputation Results**")
                                    # Check for any remaining missing values
                                    remaining_missing = current_data.isna().sum().sum()
                                    if remaining_missing > 0:
                                        st.warning(f"Imputation incomplete: {remaining_missing} values still missing")
                                    else:
                                        st.success("All missing values successfully imputed")
                            else:
                                st.warning("No imputed dataset available. Please run MCMC interpolation in the previous tab first.")
                                
                                if 'convergence_datasets' in st.session_state and st.session_state.convergence_datasets:
                                    num_datasets = len(st.session_state.convergence_datasets)
                                    st.write(f"Number of imputed datasets: {num_datasets}")
                                    
                                    # Show creation timestamps range
                                    if num_datasets > 0:
                                        timestamps = [ds['timestamp'] for ds in st.session_state.convergence_datasets]
                                        if timestamps:
                                            earliest = min(timestamps)
                                            latest = max(timestamps)
                                            st.write(f"Created between: {earliest.strftime('%Y-%m-%d %H:%M')} and {latest.strftime('%Y-%m-%d %H:%M')}")
                                else:
                                    st.write("Single imputed dataset available")
                            
                            # Add modules analysis section
                            st.subheader("Modules Analysis")
                            
                            st.write("""
                            This analysis uses the MCMC-interpolated dataset from the previous step to run multiple analytical 
                            methods and evaluate the reliability of imputed values.
                            """)
                            
                            # Reference the dataset from MCMC interpolation
                            if 'interpolated_result' in st.session_state and st.session_state.interpolated_result is not None:
                                st.success("Using MCMC-interpolated dataset from Data Processing module")
                                
                                # Initialization for convergence datasets if needed
                                if 'convergence_datasets' not in st.session_state:
                                    st.session_state.convergence_datasets = []
                                    
                                if 'convergence_iterations' not in st.session_state:
                                    st.session_state.convergence_iterations = 0
                                
                                # Add current dataset to analysis - optional step
                                if st.button("Add Current Interpolated Dataset to Analysis", key="add_dataset_btn"):
                                    # Create a copy to avoid reference issues
                                    dataset_copy = st.session_state.interpolated_result.copy()
                                    
                                    # Add dataset to convergence analysis list
                                    dataset_id = len(st.session_state.convergence_datasets) + 1
                                    dataset_info = {
                                        'id': dataset_id,
                                        'name': f"Analysis {dataset_id}",
                                        'data': dataset_copy,
                                        'convergence_scores': {},
                                        'methods': [],
                                        'timestamp': pd.Timestamp.now()
                                    }
                                    
                                    st.session_state.convergence_datasets.append(dataset_info)
                                    st.session_state.convergence_iterations += 1
                                    
                                    st.success(f"Dataset {dataset_info['id']} added to analysis.")
                                
                                # Show available analytical methods
                                st.subheader("Select Analytical Methods")
                                
                                # Define analytical methods
                                analysis_methods = [
                                    "Linear Regression Analysis",
                                    "K-Means Clustering",
                                    "PCA Factor Analysis",
                                    "Correlation Analysis",
                                    "Statistical Hypothesis Testing"
                                ]
                                
                                # Allow selection of multiple methods
                                selected_methods = st.multiselect(
                                    "Choose one or more analytical methods to apply to the dataset:",
                                    options=analysis_methods,
                                    default=analysis_methods[:2]  # Default to first two methods
                                )
                                
                                # Configure parameters for selected methods
                                if selected_methods:
                                    method_params = {}
                                    
                                    # Parameters for Linear Regression
                                    if "Linear Regression Analysis" in selected_methods:
                                        st.write("### Linear Regression Parameters")
                                        col1, col2 = st.columns(2)
                                        
                                        with col1:
                                            # Get numeric columns
                                            numeric_cols = st.session_state.interpolated_result.select_dtypes(include=np.number).columns.tolist()
                                            if numeric_cols:
                                                # Default to the last numeric column as target
                                                default_target_idx = -1
                                                target_var = st.selectbox(
                                                    "Select target variable:", 
                                                    options=numeric_cols,
                                                    index=len(numeric_cols)-1,  # Default to last column
                                                    key="lin_reg_target"
                                                )
                                                
                                                # Filter out target from features
                                                feature_options = [col for col in numeric_cols if col != target_var]
                                                
                                                # Default to the first column as feature
                                                default_features = [feature_options[0]] if feature_options else []
                                                
                                                feature_vars = st.multiselect(
                                                    "Select predictor variables:",
                                                    options=feature_options,
                                                    default=default_features,
                                                    key="lin_reg_features"
                                                )
                                                
                                                method_params["Linear Regression Analysis"] = {
                                                    "target": target_var,
                                                    "features": feature_vars
                                                }
                                            else:
                                                st.warning("No numeric columns available for regression analysis.")
                                        
                                        with col2:
                                            test_size = st.slider(
                                                "Test set size (%):", 
                                                min_value=10, 
                                                max_value=50, 
                                                value=20, 
                                                key="lin_reg_test_size"
                                            )
                                            
                                            method_params.setdefault("Linear Regression Analysis", {})["test_size"] = test_size/100
                                    
                                    # Parameters for K-Means Clustering
                                    if "K-Means Clustering" in selected_methods:
                                        st.write("### K-Means Clustering Parameters")
                                        
                                        col1, col2 = st.columns(2)
                                        
                                        with col1:
                                            # Get numeric columns for clustering
                                            numeric_cols = st.session_state.interpolated_result.select_dtypes(include=np.number).columns.tolist()
                                            
                                            # Default to the first column for clustering
                                            default_cluster_vars = [numeric_cols[0]] if numeric_cols else []
                                            
                                            cluster_vars = st.multiselect(
                                                "Select variables for clustering:",
                                                options=numeric_cols,
                                                default=default_cluster_vars,
                                                key="kmeans_features"
                                            )
                                            
                                            method_params["K-Means Clustering"] = {
                                                "features": cluster_vars
                                            }
                                        
                                        with col2:
                                            n_clusters = st.slider(
                                                "Number of clusters:", 
                                                min_value=2, 
                                                max_value=10, 
                                                value=3, 
                                                key="kmeans_clusters"
                                            )
                                            
                                            method_params.setdefault("K-Means Clustering", {})["n_clusters"] = n_clusters
                                    
                                    # Parameters for PCA
                                    if "PCA Factor Analysis" in selected_methods:
                                        st.write("### PCA Factor Analysis Parameters")
                                        
                                        col1, col2 = st.columns(2)
                                        
                                        with col1:
                                            # Get numeric columns for PCA
                                            numeric_cols = st.session_state.interpolated_result.select_dtypes(include=np.number).columns.tolist()
                                            
                                            # Default to the first column for PCA
                                            default_pca_vars = [numeric_cols[0]] if numeric_cols else []
                                            
                                            pca_vars = st.multiselect(
                                                "Select variables for PCA:",
                                                options=numeric_cols,
                                                default=default_pca_vars,
                                                key="pca_features"
                                            )
                                            
                                            method_params["PCA Factor Analysis"] = {
                                                "features": pca_vars
                                            }
                                        
                                        with col2:
                                            n_components = st.slider(
                                                "Number of components:", 
                                                min_value=2, 
                                                max_value=min(10, len(numeric_cols)), 
                                                value=min(3, len(numeric_cols)), 
                                                key="pca_components"
                                            )
                                            
                                            method_params.setdefault("PCA Factor Analysis", {})["n_components"] = n_components
                                    
                                    # Execute button
                                    if st.button("Run Selected Analytical Methods"):
                                        with st.spinner("Running analytical methods on all MCMC-generated datasets..."):
                                            # Check if we have multiple datasets from MCMC
                                            datasets_to_analyze = []
                                            
                                            # First add the current interpolated result
                                            if 'interpolated_result' in st.session_state and st.session_state.interpolated_result is not None:
                                                datasets_to_analyze.append({
                                                    'name': 'Current Interpolated Dataset',
                                                    'data': st.session_state.interpolated_result
                                                })
                                                
                                            # Then add all datasets from convergence_datasets (if any)
                                            if 'convergence_datasets' in st.session_state and st.session_state.convergence_datasets:
                                                for ds in st.session_state.convergence_datasets:
                                                    if 'data' in ds and ds['data'] is not None:
                                                        datasets_to_analyze.append({
                                                            'name': ds.get('name', f"Dataset {ds.get('id', 'unknown')}"),
                                                            'data': ds['data'],
                                                            'id': ds.get('id')
                                                        })
                                            
                                            # Check if we have datasets to analyze
                                            if not datasets_to_analyze:
                                                st.error("No datasets available for analysis. Please run MCMC interpolation first.")
                                                
                                            else:
                                                st.success(f"Found {len(datasets_to_analyze)} datasets to analyze")
                                                
                                                # Store all results
                                                all_method_results = {}
                                                
                                                # Process each dataset
                                                for dataset_info in datasets_to_analyze:
                                                    data = dataset_info['data']
                                                    dataset_name = dataset_info['name']
                                                    
                                                    st.subheader(f"Analysis for {dataset_name}")
                                                    
                                                    # Placeholder for results
                                                    method_results = {}
                                                    convergence_scores = {}
                                                    
                                                    # Run each selected method for this dataset
                                                    for method in selected_methods:
                                                        st.write(f"### Results for {method}")
                                                        
                                                        if method == "Linear Regression Analysis" and method in method_params:
                                                            params = method_params[method]
                                                            
                                                            # Get the target and features
                                                            target = params["target"]
                                                            features = params["features"]
                                                            test_size = params["test_size"]
                                                    
                                                            # Check if we have enough data
                                                            if len(features) == 0:
                                                                st.error(f"Need at least one feature for {method}.")
                                                                continue
                                                        
                                                            # Prepare the data
                                                            from sklearn.model_selection import train_test_split
                                                            X = data[features]
                                                            y = data[target]
                                                            
                                                            # Drop any rows with NaN values
                                                            valid_indices = ~(X.isna().any(axis=1) | y.isna())
                                                            X = X[valid_indices]
                                                            y = y[valid_indices]
                                                    
                                                            # Split the data
                                                            X_train, X_test, y_train, y_test = train_test_split(
                                                                X, y, test_size=test_size, random_state=42
                                                            )
                                                    
                                                            # Train the model
                                                            from sklearn.linear_model import LinearRegression
                                                            from sklearn.metrics import r2_score, mean_squared_error
                                                            
                                                            model = LinearRegression()
                                                            model.fit(X_train, y_train)
                                                            
                                                            # Make predictions
                                                            y_train_pred = model.predict(X_train)
                                                            y_test_pred = model.predict(X_test)
                                                    
                                                    # Compute metrics
                                                    train_r2 = r2_score(y_train, y_train_pred)
                                                    test_r2 = r2_score(y_test, y_test_pred)
                                                    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
                                                    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
                                                    
                                                    # Store metrics for convergence analysis
                                                    convergence_scores['regression_train_r2'] = train_r2
                                                    convergence_scores['regression_test_r2'] = test_r2
                                                    
                                                    # Display metrics
                                                    metrics_df = pd.DataFrame({
                                                        'Metric': ['R¬≤ Score', 'RMSE'],
                                                        'Train': [train_r2, train_rmse],
                                                        'Test': [test_r2, test_rmse]
                                                    })
                                                    st.dataframe(metrics_df)
                                                    
                                                    # Display coefficients
                                                    coef_df = pd.DataFrame({
                                                        'Feature': features,
                                                        'Coefficient': model.coef_
                                                    })
                                                    st.write("#### Model Coefficients")
                                                    st.dataframe(coef_df)
                                                    
                                                    # Visualize predictions vs actual
                                                    fig, ax = plt.subplots(1, 2, figsize=(12, 5))
                                                    ax[0].scatter(y_train, y_train_pred, alpha=0.5)
                                                    ax[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--')
                                                    ax[0].set_xlabel('Actual')
                                                    ax[0].set_ylabel('Predicted')
                                                    ax[0].set_title('Train Set')
                                                    
                                                    ax[1].scatter(y_test, y_test_pred, alpha=0.5)
                                                    ax[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
                                                    ax[1].set_xlabel('Actual')
                                                    ax[1].set_ylabel('Predicted')
                                                    ax[1].set_title('Test Set')
                                                    
                                                    plt.tight_layout()
                                                    st.pyplot(fig)
                                                    
                                                    # Store results
                                                    method_results[method] = {
                                                        'model': model,
                                                        'features': features,
                                                        'target': target,
                                                        'metrics': {
                                                            'train_r2': train_r2,
                                                            'test_r2': test_r2,
                                                            'train_rmse': train_rmse,
                                                            'test_rmse': test_rmse
                                                        }
                                                    }
                                                    
                                                        elif method == "K-Means Clustering" and method in method_params:
                                                            params = method_params[method]
                                                            
                                                            # Get features for clustering
                                                            features = params["features"]
                                                            n_clusters = params["n_clusters"]
                                                    
                                                            # Check if we have enough data
                                                            if len(features) < 2:
                                                                st.error(f"Need at least two features for {method}.")
                                                                continue
                                                        
                                                            # Prepare the data
                                                            from sklearn.preprocessing import StandardScaler
                                                            from sklearn.cluster import KMeans
                                                            
                                                            X = data[features].dropna()
                                                            
                                                            # Scale the data
                                                            scaler = StandardScaler()
                                                            X_scaled = scaler.fit_transform(X)
                                                            
                                                            # Perform clustering
                                                            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                                                            clusters = kmeans.fit_predict(X_scaled)
                                                    
                                                    # Add cluster labels to the original data
                                                    clustered_data = X.copy()
                                                    clustered_data['Cluster'] = clusters
                                                    
                                                    # Display clustered data
                                                    st.write("#### Sample of Clustered Data")
                                                    st.dataframe(clustered_data.head(10))
                                                    
                                                    # Display cluster statistics
                                                    st.write("#### Cluster Statistics")
                                                    cluster_stats = clustered_data.groupby('Cluster').agg(['mean', 'std'])
                                                    st.dataframe(cluster_stats)
                                                    
                                                    # Store inertia for convergence analysis
                                                    convergence_scores['kmeans_inertia'] = kmeans.inertia_
                                                    
                                                    # Visualize clusters (first two dimensions)
                                                    if len(features) >= 2:
                                                        fig, ax = plt.subplots(figsize=(10, 6))
                                                        scatter = ax.scatter(
                                                            X_scaled[:, 0], 
                                                            X_scaled[:, 1], 
                                                            c=clusters, 
                                                            cmap='viridis', 
                                                            alpha=0.6
                                                        )
                                                        
                                                        # Add cluster centers
                                                        centers = kmeans.cluster_centers_
                                                        ax.scatter(
                                                            centers[:, 0], 
                                                            centers[:, 1], 
                                                            s=200, 
                                                            marker='X', 
                                                            c='red', 
                                                            alpha=0.8
                                                        )
                                                        
                                                        ax.set_title('K-Means Clustering Results (Standardized Features)')
                                                        ax.set_xlabel(f'Feature 1: {features[0]}')
                                                        ax.set_ylabel(f'Feature 2: {features[1]}')
                                                        plt.colorbar(scatter, label='Cluster')
                                                        plt.tight_layout()
                                                        st.pyplot(fig)
                                                    
                                                    # Store results
                                                    method_results[method] = {
                                                        'model': kmeans,
                                                        'features': features,
                                                        'n_clusters': n_clusters,
                                                        'scaler': scaler,
                                                        'inertia': kmeans.inertia_,
                                                        'cluster_centers': kmeans.cluster_centers_
                                                    }
                                                    
                                                elif method == "PCA Factor Analysis" and method in method_params:
                                                    params = method_params[method]
                                                    
                                                    # Get features for PCA
                                                    features = params["features"]
                                                    n_components = params["n_components"]
                                                    
                                                    # Check if we have enough data
                                                    if len(features) < n_components:
                                                        st.error(f"Need at least {n_components} features for PCA with {n_components} components.")
                                                        continue
                                                        
                                                    # Prepare the data
                                                    from sklearn.preprocessing import StandardScaler
                                                    from sklearn.decomposition import PCA
                                                    
                                                    X = data[features].dropna()
                                                    
                                                    # Scale the data
                                                    scaler = StandardScaler()
                                                    X_scaled = scaler.fit_transform(X)
                                                    
                                                    # Perform PCA
                                                    pca = PCA(n_components=n_components)
                                                    X_pca = pca.fit_transform(X_scaled)
                                                    
                                                    # Create dataframe with PCA results
                                                    pca_df = pd.DataFrame(
                                                        X_pca, 
                                                        columns=[f'PC{i+1}' for i in range(n_components)]
                                                    )
                                                    
                                                    # Display PCA results
                                                    st.write("#### Sample of PCA Results")
                                                    st.dataframe(pca_df.head(10))
                                                    
                                                    # Display explained variance
                                                    explained_variance = pca.explained_variance_ratio_
                                                    cum_explained_variance = np.cumsum(explained_variance)
                                                    
                                                    # Store for convergence analysis
                                                    convergence_scores['pca_explained_variance'] = explained_variance
                                                    
                                                    var_df = pd.DataFrame({
                                                        'Principal Component': [f'PC{i+1}' for i in range(n_components)],
                                                        'Explained Variance (%)': explained_variance * 100,
                                                        'Cumulative Variance (%)': cum_explained_variance * 100
                                                    })
                                                    
                                                    st.write("#### Explained Variance")
                                                    st.dataframe(var_df)
                                                    
                                                    # Visualize explained variance
                                                    fig, ax = plt.subplots(figsize=(10, 6))
                                                    ax.bar(
                                                        range(1, n_components+1), 
                                                        explained_variance * 100, 
                                                        alpha=0.5, 
                                                        label='Individual'
                                                    )
                                                    ax.step(
                                                        range(1, n_components+1), 
                                                        cum_explained_variance * 100, 
                                                        where='mid', 
                                                        label='Cumulative'
                                                    )
                                                    
                                                    ax.set_xlabel('Principal Component')
                                                    ax.set_ylabel('Explained Variance (%)')
                                                    ax.set_title('Explained Variance by Principal Component')
                                                    ax.set_xticks(range(1, n_components+1))
                                                    ax.legend()
                                                    plt.tight_layout()
                                                    st.pyplot(fig)
                                                    
                                                    # Visualize first two components
                                                    if n_components >= 2:
                                                        fig, ax = plt.subplots(figsize=(10, 6))
                                                        scatter = ax.scatter(
                                                            X_pca[:, 0], 
                                                            X_pca[:, 1], 
                                                            alpha=0.6
                                                        )
                                                        
                                                        ax.set_title('First Two Principal Components')
                                                        ax.set_xlabel(f'PC1 ({explained_variance[0]:.2%} var.)')
                                                        ax.set_ylabel(f'PC2 ({explained_variance[1]:.2%} var.)')
                                                        plt.tight_layout()
                                                        st.pyplot(fig)
                                                        
                                                        # Display component loadings
                                                        loadings = pca.components_
                                                        loadings_df = pd.DataFrame(
                                                            loadings.T, 
                                                            index=features,
                                                            columns=[f'PC{i+1}' for i in range(n_components)]
                                                        )
                                                        
                                                        st.write("#### Component Loadings")
                                                        st.dataframe(loadings_df)
                                                    
                                                    # Store results
                                                    method_results[method] = {
                                                        'model': pca,
                                                        'features': features,
                                                        'n_components': n_components,
                                                        'explained_variance': explained_variance,
                                                        'loadings': pca.components_
                                                    }
                                                    
                                                elif method == "Correlation Analysis":
                                                    # Perform correlation analysis on numeric columns
                                                    numeric_data = data.select_dtypes(include=np.number)
                                                    
                                                    # Calculate correlation matrix
                                                    corr_matrix = numeric_data.corr()
                                                    
                                                    # Display correlation matrix
                                                    st.write("#### Correlation Matrix")
                                                    st.dataframe(corr_matrix)
                                                    
                                                    # Visualize correlation matrix
                                                    fig, ax = plt.subplots(figsize=(12, 10))
                                                    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
                                                    cmap = sns.diverging_palette(230, 20, as_cmap=True)
                                                    
                                                    sns.heatmap(
                                                        corr_matrix, 
                                                        mask=mask, 
                                                        cmap=cmap, 
                                                        vmax=1, 
                                                        vmin=-1, 
                                                        center=0,
                                                        square=True, 
                                                        linewidths=.5, 
                                                        annot=True, 
                                                        fmt=".2f"
                                                    )
                                                    
                                                    plt.title('Correlation Matrix')
                                                    plt.tight_layout()
                                                    st.pyplot(fig)
                                                    
                                                    # Store results
                                                    method_results[method] = {
                                                        'correlation_matrix': corr_matrix
                                                    }
                                                    
                                                    # Store for convergence
                                                    convergence_scores['correlation_mean'] = corr_matrix.abs().mean().mean()
                                                
                                                elif method == "Statistical Hypothesis Testing":
                                                    # Perform basic hypothesis tests on numeric columns
                                                    numeric_data = data.select_dtypes(include=np.number)
                                                    
                                                    # Normality tests (Shapiro-Wilk)
                                                    st.write("#### Normality Tests (Shapiro-Wilk)")
                                                    normality_results = []
                                                    
                                                    for col in numeric_data.columns:
                                                        # Drop NaN values
                                                        values = numeric_data[col].dropna()
                                                        
                                                        # Only test if we have enough data (3-5000 samples)
                                                        if len(values) >= 3 and len(values) <= 5000:
                                                            stat, p = stats.shapiro(values)
                                                            normality_results.append({
                                                                'Column': col,
                                                                'Statistic': stat,
                                                                'p-value': p,
                                                                'Normal Distribution': 'Yes' if p > 0.05 else 'No'
                                                            })
                                                        else:
                                                            normality_results.append({
                                                                'Column': col,
                                                                'Statistic': None,
                                                                'p-value': None,
                                                                'Normal Distribution': 'Not tested (insufficient samples)'
                                                            })
                                                    
                                                    st.dataframe(pd.DataFrame(normality_results))
                                                    
                                                    # Store results
                                                    method_results[method] = {
                                                        'normality_tests': normality_results
                                                    }
                                                    
                                                    # Calculate how many variables follow normal distribution
                                                    normal_count = sum(1 for result in normality_results 
                                                                    if result['Normal Distribution'] == 'Yes')
                                                    
                                                    total_tested = sum(1 for result in normality_results 
                                                                    if result['Normal Distribution'] not in ['Not tested (insufficient samples)'])
                                                    
                                                    if total_tested > 0:
                                                        normal_pct = (normal_count / total_tested) * 100
                                                        convergence_scores['normality_percentage'] = normal_pct
                                            
                                            # Save the results to the session state
                                            new_analysis = {
                                                'id': len(st.session_state.convergence_datasets) + 1,
                                                'name': f"Analysis {len(st.session_state.convergence_datasets) + 1}",
                                                'methods': selected_methods,
                                                'method_results': method_results,
                                                'convergence_scores': convergence_scores,
                                                'data': data,
                                                'timestamp': pd.Timestamp.now()
                                            }
                                            
                                            st.session_state.convergence_datasets.append(new_analysis)
                                            
                                            st.success(f"Analysis complete. Results saved as Analysis {new_analysis['id']}.")
                                            st.info("Navigate to the Convergence Evaluation tab to evaluate convergence with multiple analyses.")
                                else:
                                    st.warning("Please select at least one analytical method to continue.")
                                
                            # Evaluate Convergence section
                            if 'convergence_datasets' not in st.session_state:
                                st.session_state.convergence_datasets = []
                                
                            if not st.session_state.convergence_datasets:
                                st.warning("No analysis results available. Please run analysis in the Analyze Imputed Data tab.")
                            else:
                                st.write("### Convergence Evaluation")
                                st.write("Evaluate the convergence of multiple imputations.")
                                
                                # Show available datasets
                                st.write("#### Available Analysis Results")
                                results_df = pd.DataFrame([
                                    {
                                        'ID': a['id'],
                                        'Dataset': a['name'],
                                        'Methods': ", ".join(a.get('methods', [])),
                                        'Timestamp': a['timestamp'].strftime("%Y-%m-%d %H:%M")
                                    }
                                    for a in st.session_state.convergence_datasets
                                ])
                                
                                st.dataframe(results_df)
                                
                                # Select datasets to compare
                                selected_ids = st.multiselect(
                                    "Select analysis results to compare:",
                                    options=[a['id'] for a in st.session_state.convergence_datasets],
                                    default=[]
                                )
                                
                                if selected_ids:
                                    # Get selected analyses
                                    selected_analyses = [a for a in st.session_state.convergence_datasets if a['id'] in selected_ids]
                                    
                                    if len(selected_analyses) > 1:
                                        st.write("### Convergence Statistics")
                                        
                                        # Prepare data for comparison
                                        comparison_data = []
                                        
                                        for analysis in selected_analyses:
                                            # Extract relevant metrics from each analysis result
                                            metrics = {
                                                'ID': analysis['id'],
                                                'Dataset': analysis['name']
                                            }
                                            
                                            # Add convergence scores
                                            for key, value in analysis.get('convergence_scores', {}).items():
                                                metrics[key] = value
                                            
                                            comparison_data.append(metrics)
                                        
                                        # Display comparison table
                                        comparison_df = pd.DataFrame(comparison_data)
                                        st.dataframe(comparison_df)
                                        
                                        # Calculate variation statistics
                                        if len(comparison_data) > 1:
                                            st.write("#### Variation Between Imputations")
                                            
                                            numeric_cols = comparison_df.select_dtypes(include=np.number).columns
                                            if len(numeric_cols) > 0:
                                                # Calculate coefficient of variation (CV) for each metric
                                                variation_stats = {}
                                                
                                                for col in numeric_cols:
                                                    values = comparison_df[col].dropna()
                                                    if len(values) > 1:
                                                        mean = values.mean()
                                                        std = values.std()
                                                        cv = (std / mean) * 100 if mean != 0 else float('nan')
                                                        
                                                        variation_stats[col] = {
                                                            'Mean': mean,
                                                            'Std': std,
                                                            'CV (%)': cv
                                                        }
                                                
                                                if variation_stats:
                                                    var_df = pd.DataFrame(variation_stats).T
                                                    st.dataframe(var_df)
                                                    
                                                    # Interpret convergence
                                                    st.write("#### Convergence Interpretation")
                                                    
                                                    cv_values = var_df['CV (%)'].dropna()
                                                    if len(cv_values) > 0:
                                                        avg_cv = cv_values.mean()
                                                        
                                                        if avg_cv < 5:
                                                            st.success(f"Good convergence (Average CV: {avg_cv:.2f}%). The multiple imputations have converged well.")
                                                        elif avg_cv < 10:
                                                            st.info(f"Acceptable convergence (Average CV: {avg_cv:.2f}%). The multiple imputations show reasonable consistency.")
                                                        else:
                                                            st.warning(f"Poor convergence (Average CV: {avg_cv:.2f}%). Consider running more imputations or reviewing the imputation model.")
                                                else:
                                                    st.warning("No comparable numeric metrics found across the selected analyses.")
                                            else:
                                                st.warning("No numeric metrics available for convergence evaluation.")
                                    else:
                                        st.warning("Please select at least two analysis results to compare convergence.")
                                
                            # Add database section for storing and retrieving analysis results
                            st.write("### Save and Load Analysis Results")
                            
                            # Save current results
                            if st.button("Save Current Analysis Results to Database"):
                                if 'convergence_datasets' in st.session_state and st.session_state.convergence_datasets:
                                    try:
                                        # Use SQLAlchemy to save to database
                                        # This is a placeholder - actual implementation would depend on your database schema
                                        st.success("Analysis results saved to database.")
                                    except Exception as e:
                                        st.error(f"Error saving to database: {str(e)}")
                                else:
                                    st.warning("No analysis results available to save.")
                            
                            # Load previous results
                            st.write("#### Load Previous Analysis Results")
                            if st.button("Load Analysis Results from Database"):
                                try:
                                    # Use SQLAlchemy to load from database
                                    # This is a placeholder - actual implementation would depend on your database schema
                                    st.info("This would load previous analysis results from the database.")
                                except Exception as e:
                                    st.error(f"Error loading from database: {str(e)}")
                            
                    # End of the Modules Analysis module

if __name__ == "__main__":
    st.sidebar.info("Data Analysis Platform")
    st.sidebar.markdown("---")
    st.sidebar.markdown("A comprehensive platform for data analysis, statistical modeling, and advanced modules convergence testing.")
